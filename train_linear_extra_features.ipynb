{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from bisect import bisect_left, bisect_right\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPU hack if you need\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'linear_extra_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(df, x, title='', figsize=(18, 6), top=None, **kwargs):\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    val_cnt = df[x].value_counts()\n",
    "    val_cnt = val_cnt.iloc[:top] if top else val_cnt\n",
    "\n",
    "    sns.countplot(data=df, x=x, order=val_cnt.index, **kwargs)\n",
    "    plt.xticks(rotation=90, fontsize=12)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=16)\n",
    "    else:\n",
    "        plt.title(f'{x.capitalize()} count', fontsize=16)\n",
    "    plt.plot()\n",
    "    \n",
    "\n",
    "def calculate_product_popularity():\n",
    "    product_popularity = dict()\n",
    "\n",
    "    for df in prod_df_generator():\n",
    "        df = df[df.product_id != 0]\n",
    "        for product_id, cnt in df.product_id.value_counts().items():\n",
    "            if product_id in product_popularity:\n",
    "                product_popularity[product_id] += cnt\n",
    "            else:\n",
    "                product_popularity[product_id] = cnt\n",
    "                \n",
    "    total_cnt = sum(v for v in product_popularity.values())\n",
    "    product_popularity = {k: v/total_cnt for k, v in product_popularity.items()}\n",
    "    \n",
    "    return product_popularity\n",
    "\n",
    "\n",
    "def calcualte_user_orderes():\n",
    "    user_orderes = dict()\n",
    "\n",
    "    for df in prod_df_generator():\n",
    "        for order_id, user_id in df.groupby(['order_id', 'user_id']).size().index:\n",
    "            if user_id in user_orderes:\n",
    "                user_orderes[user_id] += 1\n",
    "            else:\n",
    "                user_orderes[user_id] = 1\n",
    "                \n",
    "    return user_orderes\n",
    "\n",
    "\n",
    "def create_user_ids():\n",
    "\n",
    "    user_ids = set()\n",
    "\n",
    "    for prod_df in prod_df_generator(usecols=['user_id', 'product_id']):\n",
    "        prod_df = prod_df[prod_df.product_id != 0]\n",
    "        prod_df.user_id = prod_df.user_id\n",
    "        user_ids = user_ids.union(prod_df.user_id.unique())\n",
    "\n",
    "    return [int(v) for v in user_ids]\n",
    "\n",
    "\n",
    "def date_parser_users(d):\n",
    "    try:\n",
    "        d = datetime.strptime(d, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            d = datetime.strptime(d, '%d.%m.%Y')\n",
    "        except:\n",
    "            d = datetime.strptime(d, '%d.%m')\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'data'\n",
    "\n",
    "PATH_TO_PRODUCTS = f'{PATH_TO_DATA}/sbermarket_tab_2_1/'\n",
    "PATH_TO_ORDERS = f'{PATH_TO_DATA}/kaggle_tab_1345/tab_1_orders.csv'\n",
    "PATH_TO_CATS = f'{PATH_TO_DATA}/kaggle_tab_1345/tab_3_categories.csv'\n",
    "PATH_TO_PROD_PROP = f'{PATH_TO_DATA}/kaggle_tab_1345/tab_5_product_properties.csv'\n",
    "PATH_TO_USERS = f'{PATH_TO_DATA}/kaggle_tab_1345/tab_4_user_profiles.csv'\n",
    "PATH_TO_CITIES = f'{PATH_TO_DATA}/tab_6_city.csv'\n",
    "PATH_TO_SMPL_SUBM = f'{PATH_TO_DATA}/sample_submission.csv'\n",
    "\n",
    "PRODUCT_TABS = os.listdir(PATH_TO_PRODUCTS)\n",
    "\n",
    "PATH_TO_PREPROC_DATA = 'preprocessed_data'\n",
    "os.makedirs(PATH_TO_PREPROC_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = '<UNK>'\n",
    "UNK_DATE = '0000-00-00 <UNK>'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DO_DUMP_FEATURES = False\n",
    "\n",
    "TOP_PRODUCTS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a_belov/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_created_time</th>\n",
       "      <th>retailer</th>\n",
       "      <th>store_id</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17431000</td>\n",
       "      <td>72</td>\n",
       "      <td>2020-09-26 10:48:57</td>\n",
       "      <td>METRO</td>\n",
       "      <td>21</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9718154</td>\n",
       "      <td>83</td>\n",
       "      <td>2020-05-08 09:46:18</td>\n",
       "      <td>METRO</td>\n",
       "      <td>87</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10056850</td>\n",
       "      <td>142</td>\n",
       "      <td>2020-05-14 15:06:03</td>\n",
       "      <td>METRO</td>\n",
       "      <td>320</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15952443</td>\n",
       "      <td>187</td>\n",
       "      <td>2020-09-01 17:34:00</td>\n",
       "      <td>ВкусВилл</td>\n",
       "      <td>533</td>\n",
       "      <td>app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10409918</td>\n",
       "      <td>224</td>\n",
       "      <td>2020-05-20 06:32:50</td>\n",
       "      <td>Ашан</td>\n",
       "      <td>183</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id   order_created_time  retailer  store_id platform\n",
       "order_id                                                           \n",
       "17431000       72  2020-09-26 10:48:57     METRO        21      app\n",
       "9718154        83  2020-05-08 09:46:18     METRO        87      web\n",
       "10056850      142  2020-05-14 15:06:03     METRO       320      app\n",
       "15952443      187  2020-09-01 17:34:00  ВкусВилл       533      app\n",
       "10409918      224  2020-05-20 06:32:50      Ашан       183      web"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retailer_threshold = 1e-3\n",
    "\n",
    "orders_df = pd.read_csv(PATH_TO_ORDERS, index_col='order_id')\n",
    "orders_df = orders_df[~orders_df.index.duplicated(keep='first')]\n",
    "\n",
    "retailer_popularity = dict((orders_df.retailer.value_counts()/len(orders_df)))\n",
    "\n",
    "most_popular_retailers = {k: v for k, v in retailer_popularity.items() if v > retailer_threshold}\n",
    "orders_df.retailer = orders_df.retailer.apply(lambda x: x if x in most_popular_retailers else UNK_TOKEN)\n",
    "\n",
    "most_popular_platforms = ['app', 'web']\n",
    "orders_df.platform = orders_df.platform.apply(lambda x: x if x in most_popular_platforms else UNK_TOKEN)\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = 2020\n",
    "min_age, max_age = 13, 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a_belov/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>bdate</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2224890</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683001</td>\n",
       "      <td>male</td>\n",
       "      <td>1987-10-11</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2102480</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224895</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930197</td>\n",
       "      <td>&lt;UNK&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender       bdate  age\n",
       "user_id                        \n",
       "2224890  <UNK>         NaN    0\n",
       "1683001   male  1987-10-11   33\n",
       "2102480  <UNK>         NaN    0\n",
       "2224895  <UNK>         NaN    0\n",
       "930197   <UNK>         NaN    0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = pd.read_csv(PATH_TO_USERS, index_col='user_id')\n",
    "users_df = users_df[~users_df.index.duplicated(keep='first')]\n",
    "\n",
    "users_df.gender = users_df.gender.fillna(UNK_TOKEN)\n",
    "\n",
    "users_df['age'] = users_df.bdate.apply(lambda x: \n",
    "                                       0 if pd.isna(x) else \n",
    "                                       current_year - date_parser_users(x).year\n",
    "                                      )\n",
    "\n",
    "users_df.age[(users_df.age < min_age) | (users_df.age > max_age)] = 0\n",
    "\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Москва</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city_name\n",
       "store_id          \n",
       "14          Москва\n",
       "21          Москва\n",
       "1           Москва\n",
       "4           Москва\n",
       "7           Москва"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df = pd.read_csv(PATH_TO_CITIES, index_col='store_id')\n",
    "cities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(values):\n",
    "    mapping = {UNK_TOKEN: 0}\n",
    "    return update_mapping(mapping, values)\n",
    "\n",
    "\n",
    "def update_mapping(mapping, values):\n",
    "    for v in values:\n",
    "        v = str(v)\n",
    "        if v != UNK_TOKEN and v not in mapping:\n",
    "            mapping[v] = len(mapping)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def prod_df_generator(usecols=None):\n",
    "    for tab in tqdm(PRODUCT_TABS):\n",
    "        if usecols:\n",
    "            yield pd.read_csv(os.path.join(PATH_TO_PRODUCTS, tab), usecols=usecols)\n",
    "        else:\n",
    "            yield pd.read_csv(os.path.join(PATH_TO_PRODUCTS, tab))\n",
    "            \n",
    "\n",
    "int_or_unk = lambda x: UNK_TOKEN if pd.isna(x) else int(x)\n",
    "\n",
    "\n",
    "def calculate_top_products():\n",
    "    prod_cnt = Counter()\n",
    "\n",
    "    for prod_df in prod_df_generator(['product_id']):\n",
    "\n",
    "        prod_df = prod_df[prod_df.product_id != 0]\n",
    "        prod_cnt.update(prod_df.product_id.values)\n",
    "\n",
    "    prod_cnt_df = pd.DataFrame.from_dict(prod_cnt, orient='index', columns=['product_cnt'])\n",
    "\n",
    "    prod_cnt_df = prod_cnt_df.sort_values(by='product_cnt', ascending=False)\n",
    "\n",
    "    return prod_cnt_df\n",
    "\n",
    "\n",
    "def select_top_product(mappings, top_products):\n",
    "    prod_mapping = {}\n",
    "    i = 1\n",
    "    for k in sorted(mappings['product_id'].keys()):\n",
    "        if k == UNK_TOKEN:\n",
    "            prod_mapping[k] = 0\n",
    "        elif int(k) in top_products:\n",
    "            prod_mapping[k] = i\n",
    "            i += 1\n",
    "        else:\n",
    "            prod_mapping[k] = 0\n",
    "\n",
    "    return prod_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a2af7a9cc948359d1fa5b6436d021c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DO_DUMP_FEATURES or True:\n",
    "    prod_cnt_df = calculate_top_products()\n",
    "    top_products = prod_cnt_df.index[:TOP_PRODUCTS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc48e1f6d4a0468a83619abf4269df51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa50ebb788c1437cab61d3eed0f8eac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [('retailer', orders_df, 'orders'),\n",
    "                ('platform', orders_df, 'orders'),\n",
    "                ('store_id', orders_df, 'orders'),\n",
    "                ('gender', users_df, 'users'),\n",
    "                ('city_name', cities_df, 'cities')\n",
    "               ]\n",
    "\n",
    "feature_cols_prod = ['brand_name', 'product_id', 'master_category_id', 'parent_category_id']\n",
    "\n",
    "if DO_DUMP_FEATURES or True:\n",
    "    mappings = defaultdict(dict)\n",
    "\n",
    "    for col, df, pref in tqdm(feature_cols):\n",
    "        col_values = df[col].astype(str)\n",
    "\n",
    "        mappings[f'{pref}_{col}'] = create_mapping(col_values.unique())\n",
    "\n",
    "    for col in feature_cols_prod:\n",
    "        mappings[col] = create_mapping([])\n",
    "\n",
    "    for prod_df in prod_df_generator(usecols=feature_cols_prod):\n",
    "        prod_df = prod_df[prod_df.product_id != 0]\n",
    "\n",
    "        brand_values = prod_df[feature_cols_prod[0]].fillna(UNK_TOKEN)\n",
    "        product_values = prod_df[feature_cols_prod[1]].fillna(UNK_TOKEN)\n",
    "        master_values = prod_df[feature_cols_prod[2]].apply(int_or_unk)\n",
    "        parent_values = prod_df[feature_cols_prod[3]].apply(int_or_unk)\n",
    "\n",
    "        mappings[feature_cols_prod[0]] = update_mapping(mappings[feature_cols_prod[0]], brand_values.unique())\n",
    "        mappings[feature_cols_prod[1]] = update_mapping(mappings[feature_cols_prod[1]], product_values.unique())\n",
    "        mappings[feature_cols_prod[2]] = update_mapping(mappings[feature_cols_prod[2]], master_values.unique())\n",
    "        mappings[feature_cols_prod[3]] = update_mapping(mappings[feature_cols_prod[3]], parent_values.unique())\n",
    "\n",
    "    mappings['product_id'] = select_top_product(mappings, top_products)    \n",
    "    \n",
    "    with open(f'{PATH_TO_PREPROC_DATA}/mappings.json', 'w') as f:\n",
    "        json.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mappings\n",
    "with open(f'{PATH_TO_PREPROC_DATA}/mappings.json', 'r') as f:\n",
    "     mappings = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DUMP_FEATURES:\n",
    "    order_dates = {int(k):v for k, v in orders_df.order_created_time.items()}\n",
    "    \n",
    "    with open(f'{PATH_TO_PREPROC_DATA}/order_dates.json', 'w') as f:\n",
    "        json.dump(order_dates, f)\n",
    "\n",
    "with open(f'{PATH_TO_PREPROC_DATA}/order_dates.json', 'r') as f:\n",
    "     order_dates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['user_id',\n",
    "                'order_id',\n",
    "                'product_id',\n",
    "                'price',\n",
    "                'quantity',\n",
    "                'discount',\n",
    "                'brand_name',\n",
    "                'master_category_id',\n",
    "                'parent_category_id'\n",
    "               ]\n",
    "\n",
    "if DO_DUMP_FEATURES:\n",
    "    user_dates = defaultdict(list)\n",
    "    user_prices = defaultdict(list)\n",
    "    user_quantities = defaultdict(list)\n",
    "    user_discounts = defaultdict(list)\n",
    "    user_brand_name = defaultdict(list)\n",
    "    user_master_category_ids = defaultdict(list)\n",
    "    user_parent_category_ids = defaultdict(list)\n",
    "    user_order_ids = defaultdict(list)\n",
    "    user_product_ids = defaultdict(list)\n",
    "\n",
    "    for prod_df in prod_df_generator(feature_cols):\n",
    "        prod_df = prod_df[prod_df.product_id != 0]\n",
    "\n",
    "        prod_df['price'] = prod_df['price'].fillna(0)\n",
    "        prod_df['quantity'] = prod_df['quantity'].fillna(1)\n",
    "        prod_df['discount'] = prod_df['discount'].fillna(0)\n",
    "        prod_df['brand_name'] = prod_df['brand_name'].fillna(UNK_TOKEN)\n",
    "        prod_df['master_category_id'] =  prod_df['master_category_id'].apply(int_or_unk).astype(str)\n",
    "        prod_df['parent_category_id'] = prod_df['parent_category_id'].apply(int_or_unk).astype(str)\n",
    "        prod_df['product_id'] = prod_df['product_id'].astype(str)\n",
    "\n",
    "        for i, row in prod_df.iterrows():\n",
    "            user_dates[row.user_id].append(order_dates.get(str(row.order_id), UNK_DATE + f' {row.order_id}'))\n",
    "            \n",
    "            user_prices[row.user_id].append(row.price)\n",
    "            user_quantities[row.user_id].append(row.quantity)\n",
    "            user_discounts[row.user_id].append(max(0, row.discount))\n",
    "            user_brand_name[row.user_id].append(mappings['brand_name'][row.brand_name])\n",
    "\n",
    "            user_master_category_ids[row.user_id].append(mappings['master_category_id'][row.master_category_id])\n",
    "            user_parent_category_ids[row.user_id].append(mappings['parent_category_id'][row.parent_category_id])\n",
    "            user_order_ids[row.user_id].append(row.order_id)\n",
    "            user_product_ids[row.user_id].append(mappings['product_id'][row.product_id])\n",
    "\n",
    "\n",
    "    pickle.dump(user_dates, open(f'{PATH_TO_PREPROC_DATA}/user_dates.pkl', 'wb'))\n",
    "    pickle.dump(user_prices, open(f'{PATH_TO_PREPROC_DATA}/user_prices.pkl', 'wb'))\n",
    "    pickle.dump(user_discounts, open(f'{PATH_TO_PREPROC_DATA}/user_discounts.pkl', 'wb'))\n",
    "    pickle.dump(user_quantities, open(f'{PATH_TO_PREPROC_DATA}/user_quantities.pkl', 'wb'))\n",
    "    pickle.dump(user_brand_name, open(f'{PATH_TO_PREPROC_DATA}/user_brand_name.pkl', 'wb'))\n",
    "    pickle.dump(user_master_category_ids, open(f'{PATH_TO_PREPROC_DATA}/user_master_category_ids.pkl', 'wb'))\n",
    "    pickle.dump(user_parent_category_ids, open(f'{PATH_TO_PREPROC_DATA}/user_parent_category_ids.pkl', 'wb'))\n",
    "    pickle.dump(user_order_ids, open(f'{PATH_TO_PREPROC_DATA}/user_order_ids.pkl', 'wb'))\n",
    "    pickle.dump(user_product_ids, open(f'{PATH_TO_PREPROC_DATA}/user_product_ids.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1.18 s, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load client data\n",
    "user_dates = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_dates.pkl', 'rb'))\n",
    "user_prices = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_prices.pkl', 'rb'))\n",
    "user_discounts = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_discounts.pkl', 'rb'))\n",
    "user_quantities = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_quantities.pkl', 'rb'))\n",
    "user_brand_name = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_brand_name.pkl', 'rb'))\n",
    "user_master_category_ids = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_master_category_ids.pkl', 'rb'))\n",
    "user_parent_category_ids = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_parent_category_ids.pkl', 'rb'))\n",
    "user_order_ids = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_order_ids.pkl', 'rb'))\n",
    "user_product_ids = pickle.load(open(f'{PATH_TO_PREPROC_DATA}/user_product_ids.pkl', 'rb'))\n",
    "\n",
    "user_ids = json.load(open(f'{PATH_TO_PREPROC_DATA}/user_ids.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcbc64c18fb48329c65df24ab5fcb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=657431.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for user_id in tqdm(user_ids):\n",
    "    args = np.argsort(user_dates[user_id])\n",
    "\n",
    "    user_prices[user_id] = np.array(user_prices[user_id])[args]\n",
    "    user_discounts[user_id] = np.array(user_discounts[user_id])[args]\n",
    "    user_quantities[user_id] = np.array(user_quantities[user_id])[args]\n",
    "    user_brand_name[user_id] = np.array(user_brand_name[user_id])[args]\n",
    "    user_master_category_ids[user_id] = np.array(user_master_category_ids[user_id])[args]\n",
    "    user_parent_category_ids[user_id] = np.array(user_parent_category_ids[user_id])[args]\n",
    "    user_order_ids[user_id] = np.array(user_order_ids[user_id])[args]\n",
    "    user_product_ids[user_id] = np.array(user_product_ids[user_id])[args]\n",
    "\n",
    "    user_dates[user_id] = np.array(user_dates[user_id])[args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 525944 Val: 131487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_user_id, valid_user_id = train_test_split(user_ids, train_size=0.8, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f'Train: {len(train_user_id)} Val: {len(valid_user_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_data(user_id):\n",
    "    user = dict()\n",
    "\n",
    "    if user_id in users_df.index:\n",
    "        user['age'] = users_df.loc[user_id].age\n",
    "        user['gender'] = mappings['users_gender'][users_df.loc[user_id].gender]\n",
    "    else:\n",
    "        user['age'] = 0\n",
    "        user['gender'] = mappings['users_gender'][UNK_TOKEN]\n",
    "\n",
    "    return user\n",
    "\n",
    "\n",
    "def get_order_data(order_id):\n",
    "    order = dict()\n",
    "\n",
    "    if order_id in orders_df.index:\n",
    "        store_id = orders_df.loc[order_id].store_id\n",
    "        city_name = cities_df.loc[store_id].city_name\n",
    "        platform = orders_df.loc[order_id].platform\n",
    "        retailer = orders_df.loc[order_id].retailer\n",
    "        \n",
    "        order['platform'] = mappings['orders_platform'][platform]\n",
    "        order['retailer'] = mappings['orders_retailer'][retailer]\n",
    "        order['city'] = mappings['cities_city_name'][city_name]\n",
    "    else:\n",
    "        order['platform'] = mappings['orders_platform'][UNK_TOKEN]\n",
    "        order['retailer'] = mappings['orders_retailer'][UNK_TOKEN]\n",
    "        order['city'] = mappings['cities_city_name'][UNK_TOKEN]\n",
    "    \n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(user_ids, is_submission=False):\n",
    "    data_prices = []\n",
    "    data_discounts = []\n",
    "    data_quantity = []\n",
    "    data_brand_name = []\n",
    "    data_master_category = []\n",
    "    data_parent_category = []\n",
    "    data_product = []\n",
    "    data_extra = []\n",
    "\n",
    "    data_labels = []\n",
    "\n",
    "    for user_id in tqdm(user_ids):\n",
    "        date_series = user_dates[user_id]\n",
    "\n",
    "        price_series = user_prices[user_id]\n",
    "        discount_series = user_discounts[user_id]\n",
    "        quantity_series = user_quantities[user_id]\n",
    "        brand_name_series = user_brand_name[user_id]\n",
    "        master_category_series = user_master_category_ids[user_id]\n",
    "        parent_category_series = user_parent_category_ids[user_id]\n",
    "\n",
    "        order_id_series = user_order_ids[user_id]\n",
    "        product_series = user_product_ids[user_id]\n",
    "\n",
    "        user = get_user_data(user_id)\n",
    "\n",
    "        if is_submission:\n",
    "            date_range = [None]\n",
    "        else:\n",
    "            date_range = np.sort(np.unique(date_series))\n",
    "\n",
    "        for date_end in date_range:\n",
    "\n",
    "            if is_submission:\n",
    "                l, r = len(date_series), len(date_series)\n",
    "            else:\n",
    "                l, r = (bisect_left(date_series, date_end),\n",
    "                        bisect_right(date_series, date_end))\n",
    "\n",
    "            history_price = price_series[:l]\n",
    "            history_discount = discount_series[:l]\n",
    "            history_quantity = quantity_series[:l]\n",
    "            history_brand_name = brand_name_series[:l]\n",
    "            history_master_category = master_category_series[:l]\n",
    "            history_parent_category = parent_category_series[:l]\n",
    "            history_product = product_series[:l]\n",
    "\n",
    "            predict_product = product_series[l:r]\n",
    "\n",
    "            if (len(predict_product) > 0) and l or is_submission:\n",
    "                data_prices.append(history_price)\n",
    "                data_discounts.append(history_discount)\n",
    "                data_quantity.append(history_quantity)\n",
    "                data_brand_name.append(history_brand_name)\n",
    "                data_master_category.append(history_master_category)\n",
    "                data_parent_category.append(history_parent_category)\n",
    "                data_product.append(history_product)\n",
    "\n",
    "                data_labels.append(predict_product)\n",
    "            \n",
    "                order_id = order_id_series[:l][-1]\n",
    "\n",
    "                order = get_order_data(order_id)\n",
    "                data_extra.append({**user, **order})\n",
    "\n",
    "    return (data_prices, data_discounts, data_quantity, data_brand_name, \n",
    "            data_master_category, data_parent_category, data_product, data_extra, data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d55ea3335f4b6cb2cc4e61f956d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=657431.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4241fa063bb547039a48033813e2769b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=131487.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(train_prices, train_discounts, train_quantity, train_brand, train_master_category,\n",
    " train_parent_category, train_product, train_extra, train_labels) = prepare_data(user_ids)\n",
    "\n",
    "(valid_prices, valid_discounts, valid_quantity, valid_brand, valid_master_category,\n",
    " valid_parent_category, valid_product, valid_extra, valid_labels) = prepare_data(valid_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_NCLASSES = len(np.unique(list(mappings['product_id'].values())))\n",
    "MASTER_CAT_NCLASSES = len(mappings['master_category_id'])\n",
    "PARENT_CAT_NCLASSES = len(mappings['parent_category_id'])\n",
    "BRAND_NCLASSES = len(mappings['brand_name'])\n",
    "PLATFORM_NCLASSES = len(mappings['orders_platform'])\n",
    "GENDER_NCLASSES = len(mappings['users_gender'])\n",
    "RETAILER_NCLASSES = len(mappings['orders_retailer'])\n",
    "CITY_NCLASSES = len(mappings['cities_city_name'])\n",
    "\n",
    "PADDING_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductHistoryDataset(Dataset):\n",
    "    def __init__(self, data_prices, data_discounts, data_quantity, data_brand_name, data_master_category,\n",
    "                 data_parent_category, data_product, data_extra, labels=None, is_submission=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_prices = data_prices\n",
    "        self.data_discounts = data_discounts\n",
    "        self.data_quantity = data_quantity\n",
    "\n",
    "        self.data_brand_name = data_brand_name\n",
    "        self.data_master_category = data_master_category\n",
    "        self.data_parent_category = data_parent_category\n",
    "        self.data_product = data_product\n",
    "        self.data_extra = data_extra\n",
    "\n",
    "        self.labels = labels\n",
    "        self.is_submission = is_submission\n",
    "        \n",
    "        self.cat_feature_names = ['brand', 'master_category', 'parent_category', 'product']\n",
    "        self.cat_features = [self.data_brand_name, self.data_master_category,\n",
    "                             self.data_parent_category, self.data_product]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_product)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        targets = np.zeros(PRODUCT_NCLASSES - 1, dtype=np.float32)\n",
    "        if not self.is_submission:\n",
    "            targets[self.labels[idx] - 1] = 1.\n",
    "\n",
    "        item = {'features': {}, 'targets': targets}\n",
    "\n",
    "        data_prices = np.array(self.data_prices[idx][-PADDING_LEN:])\n",
    "        data_prices = np.vectorize(lambda s: np.log(1 + s))(data_prices)\n",
    "\n",
    "        data_discounts = np.array(self.data_discounts[idx][-PADDING_LEN:])\n",
    "        data_discounts = np.vectorize(lambda s: np.log(1 + max(0., s)))(data_discounts) \n",
    "        data_quantity = np.array(self.data_quantity[idx][-PADDING_LEN:])\n",
    "        \n",
    "    \n",
    "        values_len = data_prices.shape[0]\n",
    "        pad = np.zeros(PADDING_LEN - values_len, dtype=np.float32)\n",
    "        \n",
    "        data_prices = np.append(data_prices, pad)\n",
    "        data_discounts = np.append(data_discounts, pad)\n",
    "        data_quantity = np.append(data_quantity, pad)\n",
    "\n",
    "        item['features']['price'] = torch.from_numpy(data_prices).float()\n",
    "        item['features']['discounts'] = torch.from_numpy(data_discounts).float()\n",
    "        item['features']['quantity'] = torch.from_numpy(data_quantity).float()\n",
    "        \n",
    "        for k, v in self.data_extra[idx].items():\n",
    "            item['features'][k] = v\n",
    "\n",
    "        for feature_name, feature_values in zip(self.cat_feature_names, [f[idx] for f in self.cat_features]):\n",
    "\n",
    "            feature_values = np.append(np.array(feature_values[-PADDING_LEN:]), pad).astype(np.int64)\n",
    "            mask = np.append(np.ones(values_len, dtype=np.float32), pad)\n",
    "\n",
    "            item['features'][feature_name] = torch.from_numpy(feature_values).long()\n",
    "            item['features'][f'{feature_name}_mask'] = torch.from_numpy(mask).float()\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProductHistoryDataset(train_prices, train_discounts, train_quantity, \n",
    "                                      train_brand, train_master_category, train_parent_category, \n",
    "                                      train_product, train_extra, train_labels)\n",
    "valid_dataset = ProductHistoryDataset(valid_prices, valid_discounts, valid_quantity, \n",
    "                                      valid_brand, valid_master_category, valid_parent_category,\n",
    "                                      valid_product, valid_extra, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, num_workers=4\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=64, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300, 2])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "torch.cat([batch['features']['price'][:2].unsqueeze(-1), \n",
    "           batch['features']['brand'][:2].unsqueeze(-1)], dim=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 612, 120, 7612, 3, 3, 7, 102)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRODUCT_NCLASSES, MASTER_CAT_NCLASSES, PARENT_CAT_NCLASSES, BRAND_NCLASSES, \\\n",
    "PLATFORM_NCLASSES, GENDER_NCLASSES, RETAILER_NCLASSES, CITY_NCLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'product_emb_dim': 300,\n",
    "          'master_cat_emb_dim': 32,\n",
    "          'parent_cat_emb_dim': 16,\n",
    "          'brand_emb_dim': 32,\n",
    "          \n",
    "          'platform_emb_dim': 2,\n",
    "          'retailer_emb_dim': 4,\n",
    "          'cuty_emb_dim': 4,\n",
    "          'gender_emb_dim': 2,\n",
    "\n",
    "          'transformer_nhead': 2,\n",
    "          'transformer_dim_feedforward': 300,\n",
    "\n",
    "          'transformer_dropout': 0.1,\n",
    "          'dense_unit': 256,\n",
    "          'num_layers': 4,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.product_embedding = nn.Embedding(PRODUCT_NCLASSES, params['product_emb_dim'])\n",
    "        self.master_cat_embedding = nn.Embedding(MASTER_CAT_NCLASSES, params['master_cat_emb_dim'])\n",
    "        self.parent_cat_embedding = nn.Embedding(PARENT_CAT_NCLASSES, params['parent_cat_emb_dim'])\n",
    "        self.brend_embedding = nn.Embedding(BRAND_NCLASSES, params['brand_emb_dim'])\n",
    "\n",
    "        self.platform_embedding = nn.Embedding(PLATFORM_NCLASSES, params['platform_emb_dim'])\n",
    "        self.retailer_embedding = nn.Embedding(RETAILER_NCLASSES, params['retailer_emb_dim'])\n",
    "        self.city_embedding = nn.Embedding(CITY_NCLASSES, params['cuty_emb_dim'])\n",
    "        self.gender_embedding = nn.Embedding(GENDER_NCLASSES, params['gender_emb_dim'])\n",
    "\n",
    "        embedding_size = (params['product_emb_dim'] + params['master_cat_emb_dim'] +\n",
    "                          params['parent_cat_emb_dim'] + params['brand_emb_dim'] +\n",
    "                          params['platform_emb_dim'] + params['retailer_emb_dim'] +\n",
    "                          params['cuty_emb_dim'] + params['gender_emb_dim'] +\n",
    "                          4\n",
    "                          )\n",
    "\n",
    "        transformer_blocks = [(f'transformer_block_{i}', \n",
    "                               nn.TransformerEncoderLayer(d_model=embedding_size,\n",
    "                                                          nhead=params['transformer_nhead'],\n",
    "                                                          dim_feedforward=params['transformer_dim_feedforward'],\n",
    "                                                          dropout=params['transformer_dropout']\n",
    "                                                         )\n",
    "                              ) for i in range(params['num_layers'])]\n",
    " \n",
    "        self.transformer_encoder = nn.Sequential(OrderedDict(transformer_blocks))\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_size, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "                                 )\n",
    "\n",
    "        self.scorer = nn.Linear(in_features=512, out_features=PRODUCT_NCLASSES - 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        product_emb = self.product_embedding(features['product'])\n",
    "        master_cat_emb = self.master_cat_embedding(features['master_category'])\n",
    "        parent_cat_emb = self.parent_cat_embedding(features['parent_category'])\n",
    "        brand_emb = self.brend_embedding(features['brand'])\n",
    "\n",
    "        product_emb = product_emb * features['product_mask'].unsqueeze(-1)\n",
    "        master_cat_emb = master_cat_emb * features['master_category_mask'].unsqueeze(-1)\n",
    "        parent_cat_emb = parent_cat_emb * features['parent_category_mask'].unsqueeze(-1)\n",
    "        brand_emb = brand_emb * features['brand_mask'].unsqueeze(-1)\n",
    "\n",
    "        platform_emb = self.platform_embedding(features['platform']).unsqueeze(1).repeat(1, PADDING_LEN, 1)\n",
    "        retailer_emb = self.retailer_embedding(features['retailer']).unsqueeze(1).repeat(1, PADDING_LEN, 1)\n",
    "        city_emb = self.city_embedding(features['city']).unsqueeze(1).repeat(1, PADDING_LEN, 1)\n",
    "        gender_emb = self.gender_embedding(features['gender']).unsqueeze(1).repeat(1, PADDING_LEN, 1)\n",
    "        \n",
    "        age = features['quantity'].unsqueeze(-1)#.repeat(1, PADDING_LEN, 1)\n",
    "        \n",
    "        #print(age.shape, brand_emb.shape, platform_emb.shape)\n",
    "        embeddings = torch.cat((product_emb, master_cat_emb, parent_cat_emb, brand_emb, \n",
    "                                platform_emb, retailer_emb, city_emb, gender_emb,\n",
    "                                features['price'].unsqueeze(-1), features['discounts'].unsqueeze(-1),\n",
    "                                features['quantity'].unsqueeze(-1),  age\n",
    "                               ), dim=-1)\n",
    "\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "\n",
    "        pooling = torch.mean(transformer_output, dim=1)\n",
    "        #print(pooling.shape)\n",
    "        \n",
    "        body = torch.tanh(self.body(pooling))\n",
    "        merch_logits = self.scorer(body)\n",
    "\n",
    "        return merch_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-batch-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7003, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Model parameters: 12602396\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "batch = next(iter(train_loader))\n",
    "output = model(batch['features'])\n",
    "loss = criterion(output, batch['targets'])\n",
    "print(loss)\n",
    "\n",
    "print('Model parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst import dl, utils\n",
    "from catalyst.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catalyst.utils.metrics.functional import preprocess_multi_label_metrics\n",
    "from catalyst.utils.torch import get_activation_fn\n",
    "\n",
    "\n",
    "def multi_label_metrics(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    threshold: Union[float, torch.Tensor],\n",
    "    activation: Optional[str] = None,\n",
    "    eps: float = 1e-7,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes multi-label precision for the specified activation and threshold.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "            indicates the probability of the example belonging to each of\n",
    "            the K classes, according to the model.\n",
    "        targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "            classes are associated with the N-th input\n",
    "            (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "            associated with classes 2 and 4)\n",
    "        threshold (float): threshold for for model output\n",
    "        activation (str): activation to use for model output\n",
    "        eps (float): epsilon to avoid zero division\n",
    "    \n",
    "    Extended version of \n",
    "        https://github.com/catalyst-team/catalyst/blob/master/catalyst/utils/metrics/accuracy.py#L58\n",
    "\n",
    "    Returns:\n",
    "        computed multi-label metrics\n",
    "    \"\"\"\n",
    "    outputs, targets, _ = preprocess_multi_label_metrics(\n",
    "        outputs=outputs, targets=targets\n",
    "    )\n",
    "    activation_fn = get_activation_fn(activation)\n",
    "    outputs = activation_fn(outputs)\n",
    "\n",
    "    outputs = (outputs > threshold).long()\n",
    "\n",
    "    accuracy = (targets.long() == outputs.long()).sum().float() / np.prod(\n",
    "        targets.shape\n",
    "    )\n",
    "\n",
    "    intersection = (outputs.long() * targets.long()).sum(axis=1).float()\n",
    "    num_predicted = outputs.long().sum(axis=1).float()\n",
    "    num_relevant = targets.long().sum(axis=1).float()\n",
    "    union = num_predicted + num_relevant\n",
    "\n",
    "    # Precision = ({predicted items} && {relevant items}) / {predicted items}\n",
    "    precision = intersection / (num_predicted + eps * (num_predicted == 0))\n",
    "    # Recall = ({predicted items} && {relevant items}) / {relevant items}\n",
    "    recall = intersection / (num_relevant + eps * (num_relevant == 0))\n",
    "    # IoU = ({predicted items} && {relevant items}) / ({predicted items} || {relevant items})\n",
    "    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n",
    "\n",
    "    return accuracy, precision.mean(), recall.mean(), iou.mean()\n",
    "\n",
    "\n",
    "def precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "       actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "       predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       k (int): parameter k of precison@k\n",
    "\n",
    "    Returns:\n",
    "       Computed value of precision@k for given sample\n",
    "    \"\"\"\n",
    "    p_at_k = 0.0\n",
    "    for item in predicted[:k]:\n",
    "        if actual[item]:\n",
    "            p_at_k += 1\n",
    "    p_at_k /= k\n",
    "\n",
    "    return p_at_k\n",
    "\n",
    "\n",
    "def average_precision_at_k(\n",
    "    actual: torch.Tensor, \n",
    "    predicted: torch.Tensor, \n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes average precision at cutoff k for one sample\n",
    "\n",
    "    Args:\n",
    "      actual: (torch.Tensor): tensor of length K with predicted item_ids sorted by relevance\n",
    "      predicted (torch.Tensor): binary tensor that encodes which of the K\n",
    "          classes are associated with the N-th input\n",
    "          (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "          associated with classes 2 and 4)\n",
    "      k (int): parameter k of AP@k\n",
    "\n",
    "    Returns:\n",
    "        Computed value of AP@k for given sample\n",
    "    \"\"\"\n",
    "    ap_at_k = 0.0\n",
    "    for idx, item in enumerate(predicted[:k]):\n",
    "        if actual[item]:\n",
    "            ap_at_k += precision_at_k(actual, predicted, k=idx + 1)\n",
    "    ap_at_k /= min(k, actual.sum().cpu().numpy())\n",
    "    \n",
    "\n",
    "    return ap_at_k\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(\n",
    "    output: torch.Tensor, target: torch.Tensor, top_k: Tuple[int, ...] = (1,)\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes mean_average_precision_at_k at set of cutoff parameters K\n",
    "\n",
    "    Args:\n",
    "       outputs (torch.Tensor): NxK tensor that for each of the N examples\n",
    "           indicates the probability of the example belonging to each of\n",
    "           the K classes, according to the model.\n",
    "       targets (torch.Tensor): binary NxK tensort that encodes which of the K\n",
    "           classes are associated with the N-th input\n",
    "           (eg: a row [0, 1, 0, 1] indicates that the example is\n",
    "           associated with classes 2 and 4)\n",
    "       top_k (tuple): list of parameters k at which map@k will be computed\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       List of computed values of map@k at each cutoff k from topk\n",
    "    \"\"\"\n",
    "    max_k = max(top_k)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, top_indices = output.topk(k=max_k, dim=1, largest=True, sorted=True)\n",
    "\n",
    "    result = []\n",
    "    for k in top_k:  # loop over k\n",
    "        map_at_k = 0.0\n",
    "        for actual_target, predicted_items in zip(\n",
    "            target, top_indices\n",
    "        ):  # loop over samples\n",
    "            map_at_k += average_precision_at_k(\n",
    "                actual_target, predicted_items, k\n",
    "            )\n",
    "        map_at_k = map_at_k / batch_size\n",
    "        result.append(map_at_k)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        features, targets = batch['features'], batch['targets']\n",
    "        logits = self.model(features)\n",
    "        scores = torch.sigmoid(logits)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "        accuracy, precision, recall, iou = multi_label_metrics(logits, targets, \n",
    "                                                               threshold=0.5, activation='Sigmoid'\n",
    "                                                              )\n",
    "        (map50, ) = mean_average_precision_at_k(scores, targets, top_k=(50,))\n",
    "        batch_metrics = {'loss': loss,\n",
    "                         'precision': precision,\n",
    "                         'recall': recall,\n",
    "                         'map50': map50,   \n",
    "                       }\n",
    "\n",
    "        self.input = {'features': features, 'targets': targets}\n",
    "        self.output = {'logits': logits, 'scores': scores}\n",
    "        self.batch_metrics.update(batch_metrics)\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def predict_batch(self, batch):\n",
    "        # model inference step\n",
    "        batch = utils.maybe_recursive_call(batch, 'to', device=self.device)\n",
    "        logits = self.model(batch['features'])\n",
    "        scores = torch.sigmoid(logits)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "model = Model()\n",
    "#model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30 * Epoch (train):   2% 516/24237 [02:01<1:33:53,  4.21it/s, loss=0.013, map50=0.087, precision=0.984, recall=0.071]   "
     ]
    }
   ],
   "source": [
    "runner = CustomRunner()\n",
    "\n",
    "runner.train(model=model,\n",
    "             criterion=criterion,\n",
    "             optimizer=optimizer,\n",
    "             scheduler=None,\n",
    "             loaders=loaders,\n",
    "             logdir=f'./logs/{model_type}',\n",
    "             num_epochs=30,\n",
    "             verbose=True,\n",
    "             load_best_on_end=True,\n",
    "             overfit=False,  #  <<<--- DO NOT FORGET TO MAKE IT ``False`` \n",
    "                             #  (``True`` uses only one batch to check pipeline correctness)\n",
    "             callbacks=[\n",
    "                 # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n",
    "                 # dl.AveragePrecisionCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"ap\"),\n",
    "                 # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "                 # dl.AUCCallback(input_key=\"targets\", output_key=\"scores\", prefix=\"auc\"),\n",
    "             ],\n",
    "\n",
    "             main_metric='iou', \n",
    "             minimize_metric=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_sbm = pd.read_csv(PATH_TO_SMPL_SUBM, index_col='Id')\n",
    "submit_ids = pd_sbm.index.values\n",
    "pd_sbm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prices, *_ = prepare_data(submit_ids, is_submission=True)\n",
    "submit_ids_with_data_idx = [i for i, tp in enumerate(test_prices) if len(tp)]\n",
    "submit_ids_with_data = submit_ids[submit_ids_with_data_idx]\n",
    "\n",
    "(test_prices, test_discounts, test_quantity, test_brand, \n",
    " test_master_category, test_parent_category, test_product, test_labels) = prepare_data(submit_ids_with_data, \n",
    "                                                                                       is_submission=True)\n",
    "\n",
    "full_dataset = ProductHistoryDataset(test_prices, test_discounts, test_quantity, \n",
    "                                     test_brand, test_master_category, test_parent_category,\n",
    "                                     test_product, is_submission=True)\n",
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False, num_workers=8, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for scores in tqdm(runner.predict_loader(loader=full_loader), total = len(full_loader)):\n",
    "    _, top_indices = scores.topk(k=50, dim=1, largest=True, sorted=True)\n",
    "    top_indices += 1\n",
    "    predictions += top_indices.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse mapping for merchant_type in predictions\n",
    "product_inverse_mapping = {k: int(v) for v, k in mappings['product_id'].items() if v != UNK_TOKEN}\n",
    "\n",
    "def inverse_mapping(x):\n",
    "    return list(map(product_inverse_mapping.get, x))\n",
    "\n",
    "predictions_prod_id = list(map(inverse_mapping, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd_sbm.copy()\n",
    "\n",
    "for i, p in tqdm(zip(submit_ids_with_data, predictions_prod_id)):\n",
    "    submission.loc[i].Predicted = ' '.join(map(str, p))\n",
    "\n",
    "sumb_path = f'submission_Hugs_for_Bugs_{model_type}.csv'\n",
    "submission.to_csv(sumb_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit test-recsys -f $sumb_path -m $model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
